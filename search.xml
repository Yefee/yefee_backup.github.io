<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[xMCA]]></title>
    <url>%2F2019%2F09%2F18%2FxMCA%2F</url>
    <content type="text"><![CDATA[About xMCAxMCA, inspired from EOFs https://github.com/ajdawson/eofs, is Maximum Covariance Analysis (sometimes also called SVD) in xarray. API Documentation: http://xmca.readthedocs.io/ How to installvia git123git clone https://github.com/Yefee/xMCA.gitcd xMCApython setup.py install Future PlanIn next version, A Monte Carlo method will be added for statistical test. Example 1MCA analysis for US surface air temperature and SST over the Pacific.This example is taken from https://atmos.washington.edu/~breth/classes/AS552/matlab/lect/html/MCA_PSSTA_USTA.html 1234from xMCA import xMCAimport xarray as xrimport matplotlib.pyplot as plt%matplotlib inline 123usta = xr.open_dataarray('xMCA/examples/data/USTA.nc').transpose(*['time', 'lat', 'lon'])usta.name = 'USTA'print(usta) &lt;xarray.DataArray &apos;USTA&apos; (time: 396, lat: 5, lon: 12)&gt; array([[[-0.450303, -0.734848, ..., -4.270303, -2.69697 ], [ 1.066061, 2.691515, ..., -4.947273, -3.330303], ..., [ nan, -0.342424, ..., nan, nan], [ nan, nan, ..., nan, nan]], [[ 1.524545, 1.370606, ..., -1.430303, 0.048485], [ 1.366364, 2.497273, ..., -0.593939, -0.079697], ..., [ nan, 0.695455, ..., nan, nan], [ nan, nan, ..., nan, nan]], ..., [[ 1.077879, 0.630303, ..., -1.262727, -1.496364], [ 1.020606, 0.114848, ..., -0.786667, -0.573939], ..., [ nan, 1.65 , ..., nan, nan], [ nan, nan, ..., nan, nan]], [[ 1.768182, 2.807879, ..., 0.885758, 0.618182], [ 1.555152, 3.435152, ..., -0.416667, 0.185152], ..., [ nan, 0.012121, ..., nan, nan], [ nan, nan, ..., nan, nan]]]) Coordinates: * lat (lat) float64 47.5 42.5 37.5 32.5 27.5 * lon (lon) float64 -122.5 -117.5 -112.5 -107.5 ... -77.5 -72.5 -67.5 * time (time) int64 0 1 2 3 4 5 6 7 8 ... 388 389 390 391 392 393 394 395 123sstpc = xr.open_dataarray('xMCA/examples/data/SSTPac.nc').transpose(*['time', 'lat', 'lon'])sstpc.name = 'SSTPC'print(sstpc) &lt;xarray.DataArray &apos;SSTPC&apos; (time: 396, lat: 30, lon: 84)&gt; [997920 values with dtype=float64] Coordinates: * lat (lat) int16 -29 -27 -25 -23 -21 -19 -17 ... 17 19 21 23 25 27 29 * lon (lon) uint16 124 126 128 130 132 134 ... 280 282 284 286 288 290 * time (time) int64 0 1 2 3 4 5 6 7 8 ... 388 389 390 391 392 393 394 395 Decompsition and retrieve the first and second loadings and expansion coefficeints123456789101112'''decomposition, time should be in the first axislp is for SSTPCrp is for USTA'''sst_ts = xMCA(sstpc, usta)sst_ts.solver()lp, rp = sst_ts.patterns(n=2)le, re = sst_ts.expansionCoefs(n=2)frac = sst_ts.covFracs(n=2)print(frac) &lt;xarray.DataArray &apos;frac&apos; (n: 2)&gt; array([0.407522, 0.391429]) Coordinates: * n (n) int64 0 1 Attributes: long_name: Fractions explained of the covariance matrix between SSTPC an... 123456fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12, 5))lp[0].plot(ax=ax1[0])le[0].plot(ax=ax1[1])rp[0].plot(ax=ax2[0])re[0].plot(ax=ax2[1]) Homogeneous and heterogeneous regression12lh, rh = sst_ts.homogeneousPatterns(n=1)le, re = sst_ts.heterogeneousPatterns(n=1) 123456fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12, 5))lh[0].plot(ax=ax1[0])rh[0].plot(ax=ax1[1])le[0].plot(ax=ax2[0])re[0].plot(ax=ax2[1]) Two-tailed Student-t test1le, re, lphet, rphet = sst_ts.heterogeneousPatterns(n=1, statistical_test=True) 1234567fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12, 5))le[0].plot(ax=ax1[0])re[0].plot(ax=ax1[1])# Only plot where p&lt;0.01lphet[0].where(lphet[0]&lt;0.01).plot(ax=ax2[0])rphet[0].where(rphet[0]&lt;0.01).plot(ax=ax2[1]) Example 2EOF analysis for US surface air temperature and SST over the PacificThis example is taken from https://atmos.washington.edu/~breth/classes/AS552/matlab/lect/html/MCA_PSSTA_USTA.html 1234from xMCA import xMCAimport xarray as xrimport matplotlib.pyplot as plt%matplotlib inline 123sstpc = xr.open_dataarray('data/SSTPac.nc').transpose(*['time', 'lat', 'lon'])sstpc.name = 'SSTPC'print(sstpc) &lt;xarray.DataArray &apos;SSTPC&apos; (time: 396, lat: 30, lon: 84)&gt; [997920 values with dtype=float64] Coordinates: * lat (lat) int16 -29 -27 -25 -23 -21 -19 -17 ... 17 19 21 23 25 27 29 * lon (lon) uint16 124 126 128 130 132 134 ... 280 282 284 286 288 290 * time (time) int64 0 1 2 3 4 5 6 7 8 ... 388 389 390 391 392 393 394 395 Decompsition and retrieve the first and second loadings and expansion coefficeints 123456789101112'''decomposition, time should be in the first axislp is for SSTPCrp is for USTA'''sst_ts = xMCA(sstpc, sstpc.rename('SSTPC_copy'))sst_ts.solver()lp, _ = sst_ts.patterns(n=2)le, _ = sst_ts.expansionCoefs(n=2)frac = sst_ts.covFracs(n=2)print(frac) &lt;xarray.DataArray &apos;frac&apos; (n: 2)&gt; array([0.873075, 0.04946 ]) Coordinates: * n (n) int64 0 1 Attributes: long_name: Fractions explained of the covariance matrix between SSTPC an... 123fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))lp[0].plot(ax=ax1[0])le[0].plot(ax=ax1[1]) Regress PC1 to the original SST field 1lh, _ = sst_ts.homogeneousPatterns(n=1) 12fig, ax1= plt.subplots()lh[0].plot(ax=ax1) About xMCAxMCA, inspired from EOFs https://github.com/ajdawson/eofs, is Maximum Covariance Analysis (sometimes also called SVD) in xarray. API Documentation: http://xmca.readthedocs.io/ How to installvia git123git clone https://github.com/Yefee/xMCA.gitcd xMCApython setup.py install Future PlanIn next version, A Monte Carlo method will be added for statistical test. Example 1MCA analysis for US surface air temperature and SST over the Pacific.This example is taken from https://atmos.washington.edu/~breth/classes/AS552/matlab/lect/html/MCA_PSSTA_USTA.html 1234from xMCA import xMCAimport xarray as xrimport matplotlib.pyplot as plt%matplotlib inline 123usta = xr.open_dataarray('xMCA/examples/data/USTA.nc').transpose(*['time', 'lat', 'lon'])usta.name = 'USTA'print(usta) &lt;xarray.DataArray &apos;USTA&apos; (time: 396, lat: 5, lon: 12)&gt; array([[[-0.450303, -0.734848, ..., -4.270303, -2.69697 ], [ 1.066061, 2.691515, ..., -4.947273, -3.330303], ..., [ nan, -0.342424, ..., nan, nan], [ nan, nan, ..., nan, nan]], [[ 1.524545, 1.370606, ..., -1.430303, 0.048485], [ 1.366364, 2.497273, ..., -0.593939, -0.079697], ..., [ nan, 0.695455, ..., nan, nan], [ nan, nan, ..., nan, nan]], ..., [[ 1.077879, 0.630303, ..., -1.262727, -1.496364], [ 1.020606, 0.114848, ..., -0.786667, -0.573939], ..., [ nan, 1.65 , ..., nan, nan], [ nan, nan, ..., nan, nan]], [[ 1.768182, 2.807879, ..., 0.885758, 0.618182], [ 1.555152, 3.435152, ..., -0.416667, 0.185152], ..., [ nan, 0.012121, ..., nan, nan], [ nan, nan, ..., nan, nan]]]) Coordinates: * lat (lat) float64 47.5 42.5 37.5 32.5 27.5 * lon (lon) float64 -122.5 -117.5 -112.5 -107.5 ... -77.5 -72.5 -67.5 * time (time) int64 0 1 2 3 4 5 6 7 8 ... 388 389 390 391 392 393 394 395 123sstpc = xr.open_dataarray('xMCA/examples/data/SSTPac.nc').transpose(*['time', 'lat', 'lon'])sstpc.name = 'SSTPC'print(sstpc) &lt;xarray.DataArray &apos;SSTPC&apos; (time: 396, lat: 30, lon: 84)&gt; [997920 values with dtype=float64] Coordinates: * lat (lat) int16 -29 -27 -25 -23 -21 -19 -17 ... 17 19 21 23 25 27 29 * lon (lon) uint16 124 126 128 130 132 134 ... 280 282 284 286 288 290 * time (time) int64 0 1 2 3 4 5 6 7 8 ... 388 389 390 391 392 393 394 395 Decompsition and retrieve the first and second loadings and expansion coefficeints123456789101112'''decomposition, time should be in the first axislp is for SSTPCrp is for USTA'''sst_ts = xMCA(sstpc, usta)sst_ts.solver()lp, rp = sst_ts.patterns(n=2)le, re = sst_ts.expansionCoefs(n=2)frac = sst_ts.covFracs(n=2)print(frac) &lt;xarray.DataArray &apos;frac&apos; (n: 2)&gt; array([0.407522, 0.391429]) Coordinates: * n (n) int64 0 1 Attributes: long_name: Fractions explained of the covariance matrix between SSTPC an... 123456fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12, 5))lp[0].plot(ax=ax1[0])le[0].plot(ax=ax1[1])rp[0].plot(ax=ax2[0])re[0].plot(ax=ax2[1]) Homogeneous and heterogeneous regression12lh, rh = sst_ts.homogeneousPatterns(n=1)le, re = sst_ts.heterogeneousPatterns(n=1) 123456fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12, 5))lh[0].plot(ax=ax1[0])rh[0].plot(ax=ax1[1])le[0].plot(ax=ax2[0])re[0].plot(ax=ax2[1]) Two-tailed Student-t test1le, re, lphet, rphet = sst_ts.heterogeneousPatterns(n=1, statistical_test=True) 1234567fig, (ax1, ax2) = plt.subplots(2, 2, figsize=(12, 5))le[0].plot(ax=ax1[0])re[0].plot(ax=ax1[1])# Only plot where p&lt;0.01lphet[0].where(lphet[0]&lt;0.01).plot(ax=ax2[0])rphet[0].where(rphet[0]&lt;0.01).plot(ax=ax2[1]) Example 2EOF analysis for US surface air temperature and SST over the PacificThis example is taken from https://atmos.washington.edu/~breth/classes/AS552/matlab/lect/html/MCA_PSSTA_USTA.html 1234from xMCA import xMCAimport xarray as xrimport matplotlib.pyplot as plt%matplotlib inline 123sstpc = xr.open_dataarray('data/SSTPac.nc').transpose(*['time', 'lat', 'lon'])sstpc.name = 'SSTPC'print(sstpc) &lt;xarray.DataArray &apos;SSTPC&apos; (time: 396, lat: 30, lon: 84)&gt; [997920 values with dtype=float64] Coordinates: * lat (lat) int16 -29 -27 -25 -23 -21 -19 -17 ... 17 19 21 23 25 27 29 * lon (lon) uint16 124 126 128 130 132 134 ... 280 282 284 286 288 290 * time (time) int64 0 1 2 3 4 5 6 7 8 ... 388 389 390 391 392 393 394 395 Decompsition and retrieve the first and second loadings and expansion coefficeints 123456789101112'''decomposition, time should be in the first axislp is for SSTPCrp is for USTA'''sst_ts = xMCA(sstpc, sstpc.rename('SSTPC_copy'))sst_ts.solver()lp, _ = sst_ts.patterns(n=2)le, _ = sst_ts.expansionCoefs(n=2)frac = sst_ts.covFracs(n=2)print(frac) &lt;xarray.DataArray &apos;frac&apos; (n: 2)&gt; array([0.873075, 0.04946 ]) Coordinates: * n (n) int64 0 1 Attributes: long_name: Fractions explained of the covariance matrix between SSTPC an... 123fig, ax1 = plt.subplots(1, 2, figsize=(12, 5))lp[0].plot(ax=ax1[0])le[0].plot(ax=ax1[1]) Regress PC1 to the original SST field 1lh, _ = sst_ts.homogeneousPatterns(n=1) 12fig, ax1= plt.subplots()lh[0].plot(ax=ax1)]]></content>
      <tags>
        <tag>data analysis, machine learning,</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Bayesian model averaging to predict global wamring at the end of 21st century]]></title>
    <url>%2F2018%2F12%2F18%2FBMA%2F</url>
    <content type="text"><![CDATA[How warm the future will be and how certain we areThe ongoing warming is a well-accepted feature of the near future climate change, however how warm it will be and how certain we are remain debatable and unclear, even though significant progress has been made in qualifying the increase in the temperature by averaging an bunch of start-of-the-art climate models (e.g. the CMIP5) or an ensemble of member forecasts from a single model (e.g. CESM-LEN). Here I provide modeling evidence that reveals the increase in temperature and its uncertainty based on Bayesian Inference. Basically, members from an ensemble model prediction is exxtracted randomly, then they are regressed on historical observation data using Bayesian linear regression. Finally these Bayesian models are averaged by stacking the predictive distributions. The results show that the stacking model can largely remove the error in the testing period and predict that the global mean surface air temperature at the end of this century is about 3 oC warmer in RCP8.5 scenario, not as intense as that of the ensemble mean, which warms about 4 oC. The warming mostly occurs at high latitudes but with large certainty which shall be taken into serious consideration by policy makers. MethodologyThe BMA method for calibrating forecast ensembles was first introduced by Raftery et al (2008). The BMA predictive PDF of a future weather quantity $x$ is the weighted sum of individual PDFs corresponding to the ensemble members. $$p \left(x|F_1,…,FM \right)= \sum{k=1}^{M}{w{k}g{k}(x|F_k)}$$ where $g_k (x│F_k )$ can be interpreted as the conditional PDF of $x$ conditional on the forecast $F_k$ and weight $w_k$ is the probability of ensemble member k being the best and is based on forecast $Fk$ s performance in the training period. The weights form a probability distribution such that $\sum{{k=1}^{M}w_k} = 1$ and $w_k≥0$. Generally, the annual mean temperature follows the Gaussian distribution centered at a linear function of the forecasts in the ensemble $μ_k$, hence the probabilistic notation of forecast $F_k$ is$$F_k | (f_1, …,f_k) ~ N(\mui_k, \sigma_k^2)$$ with the mean given by $$ \mui_k = \beta0 + \sum{i=1}^{k}{\beta_i f_i}$$ in which $(f_1,…,fk)$ denotes $k$ forecast members picked up from the ensemble set of size $K$, so that in total we would have a number of $$\sum{i=1}^{K}(i K)$$forecasts. For example, if we have an ensemble of 3 members, we can produce ∑_i^3▒(■(i@3)) =7 forecasts from this ensemble. Previously, people infer the forecast PDF g_k (x│f_k ) based on single member f_k alone which may distort the BMA production due to the chaos of the climate system. Here, I randomly combine a subset of the ensemble members, eventually building a super ensemble of single models (supBMA), which benefits the model generalization ability in the production. The Markov chain Monte Carlo6 (MCMC) is used to estimate the parameters of βs and σs. The MCMC sampler draws parameter values from the prior distribution and computes the likelihood that the observed data comes from a distribution with these parameter values.p(β│Data)∝ p(Data┤|β)*p(β)This calculation acts as a guiding light for the MCMC sampler. As it draws values from the parameter priors, it computes the likelihood of these parameters given the data, and tries to guide the sampler towards areas of higher probability. Finally, the weights wk are calculated by stacking of predictive distributions, in which several models are combined into a meta-model in order to minimize the diverge between the meta-model and the true generating model as suggested by Yao et al.7, when using a logarithmic score this is equivalently to:max┬w⁡〖1/n ∑(i=1)^n▒〖log∑_(k=1)^M▒〖w_k g_k (yi |y(-i),Mk)〗〗〗 s.t.∑(k=1)^M▒w_k =1, w_k≥0where the quantity g_k (yi |y(-i),M_k) is the leave-one-out predictive distribution for the M_k model. Case 1: Lorenz 63 toy modelI first examine my model using a low-order spectral model based on Lorenz (1963), in which parameters are set for the models as suggested by Krishnamurti et al.8. The system isdescribed by the equations below:X ̇= -σX+σY+f cos⁡θY ̇= -XZ+rX-Y+f sin⁡θZ ̇= -XY-bZin which X, Y, and Z are the time-dependent spectral amplitudes, the dot on the top of them denotes time derivative. Assignment of different values for these parameters (σ,θ,r,b) enable us to construct an ensemble of models as shown in table 1. The initial values of X, Y, and Z are 0, 10, and 0 respectively. Table 1. Toy model parameters| parameters | Control run | Multimodel parameters || :— | :—-: | —: || σ | 40.0 | 20.0-50.0 || θ | 45o | 42.3o-46.8o || r | 24.74 | 22.7-28.1 || b | 4.74 | 4.9-6.0 | This toy numerical model that can be integrated forward in time and the variation of model parameters in Table 1 can be thought of a set of model ensembles which use different physical parameterizations. The model outputs from the toy model and the supBMA prediction are shown in Figure 1a-c. The time from 0 to 150 is used for training and thereafter, all supBMA model parameters are kept as constant for the prediction in 151-200. The thick black line indicates the control run and the thin grey lines represent the ensemble of our parameter-perturbed individual model runs. It is expected that the individual models are of large error since their solutions carry large phase mismatch. All blue lines are forecasts F_k from the single Bayesian linear regression model, which has ∑_i^5▒(■(i@5)) = realizations in this case. As demonstrated by the three variable predictions, the BLR forecast reduces the error significantly compared to the individual model forecasts, especially in the first two variable forecasts. Finally, the red line indicates the supBMA prediction, with the semi-transparent shading area as the 95% confidence interval. It is shown that the supBMA is the best solution closest to the control run. Figure 1d-f shows the predictive PDF for BLR models F_k and predictions of member models f_k at time 160. The grey dots are single predictions with large spread, and the blue curves are PDF for BMA models. The red curve is PDF for supBMA model which shows small variance, and the average of the model is close to the control run represented by the vertical black line. Figure 1. The toy model predictions and the forecasts by BLR and supBMA. a-c, the control toy model prediction (black), parameter-perturbed individual model runs (grey), BLR forecasts (blue) and supBMA forecasts (red). The red interval in a-c indicates the 95% confidence interval of the supBMA prediction. d-f, the PDF of BLR (blue) and supBMA (red) predictions at time 160. The member model predictions are indicated by grey dots and the expectation of the supBMA is represented by the vertical red line with control run as the vertical black line. Case 2: Prediction of SAT at the end of the 21st centuryIn this case, I implement our supBMA model to the global SAT prediction in the 21st century. Here 8 members are randomly selected from the Community Earth System Model (CESM) large ensemble simulations provided by the CESM community. The CESM uses a nominal ~1-degree resolution globally with diagnostic biogeochemistry calculations for the ocean ecosystem and an active atmospheric carbon dioxide cycle9. Each ensemble runs from 1920 to 2100 using historical forcing (1920–2005) and Representative Concentration Pathway 85 (RCP 8.5) in 2006 to 2100, but with slightly different initial conditions. The 20th century reanalysis from National Oceanic and Atmospheric Administration is used as observation. The reanalysis covers data from 1851 to 2014, so the data from 1920 to 2014 is employed to train and test my model.Figure 2. Global mean SAT time series. The observation data covers from 1920-2014 (blue). The eight ensemble members are in grey, with the ensemble mean as red. The supBMA prediction is in green, and the error bars on supBMA shows the 90% confidence interval of the prediction. Figure 2 shows the global mean SAT from 1920s to 2100. During the training period within 1920 to 2007, the global mean SAT increases by more than 0.5 oC (blue, observation). The ensemble average of CESM (red) shows consistent response with the observations in the training period, however, the ensemble average simulation warms more strongly and rapidly than the observation during the test period (2008-2014). The same error can also be seen from Figure 3b, in which the time mean spatial errors of ensemble average simulation during 2008 to 2014 are shown. Interestingly, the ensemble average simulates a cooler climate at high latitudes but a warmer one in low latitudes mainly dominated by the response over ocean, indicating the SAT polar amplification is much more intensive in the real world than that in the model simulations. Back to the supBMA prediction, the error in ensemble members is greatly removed globally in Figure 3a. Consistently, the global mean SAT by supBMA (green) in Figure 1 is much closer to the observation (blue) in testing period than the widely used ensemble mean. Figure 3. Time mean error of supBMA and ensemble mean simulations in the testing period (2008-2014). a, supBMA. b, ensemble mean. Finally, I freeze the parameters in supBMA and apply them to the future prediction from 2015 to 2100 (I don’t have 2015-2018 observation, so here I count them as future prediction). Compared to the ensemble average with really small standard deviation as indicated by the spread of single members, the prediction from supBMA shows less increase in global mean SAT (Figure 2). However, the uncertainty measured by the 90% confidence interval at the end of 21st century amplifies compared to that in the training period as expected. Figure 4. Temperature responses at the last decade of the 21st century (2191-2100) predicted by supBMA. The dot hatching area indicates uncertainty of warming is below 2 oC, and the dash hatching area indicates uncertainty is more than 5 oC. The uncertainty is defined as the 90% confidence interval of the supBMA prediction. Figure 4 shows the spatial warming responses at the last decade of 21st century (2191-2100). The intense warming mostly occurs at high latitudes in the northern hemisphere with small uncertainty which is physically reasonable because the reduction in sea ice coverage reduces the local planet albedo and in turn the surface absorbs more short-wave radiation, eventually leading larger warming there. Other areas like west America and Sahara in Africa are also exposed into large warming danger that will bring more intensive disasters in the future. Another interesting issue need to be addressed here is that we observe cooling happens over the Southern Ocean, poleward of the Antarctic Circumpolar Current, whereas global mean SAT increases more than 3oC until the end of this century. This issue is of large certainty in my supBMA model prediction, and is also found in many other GCM simulations, but remains debatable10. Globally, most of the areas are of small uncertainty that the warming/cooling predicted by the supBMA model will happen within less than 2 oC error, especially areas at high latitudes. The most uncertain warming actually comes from the ocean surface and the Antarctic continent. DiscussionIn summary, a Bayesian statistical model is built in my current work to predict the temperature at the end of this century based on an ensemble forecast from the climate model called CESM. After training the Bayesian on the historical data, the model significantly reduces the error in the testing data compared to the ensemble mean. Finally, the model is applied to predict temperature from 2015 to 2100 and I find the Bayesian model show less temperature increase than the ensemble mean which is a good news! However, significant warming is found at highlatitudes in north hemisphere with less uncertainty which is physically reasonable. This warming shall be taken into serious consideration by the people there and policy makers to avoid the fatal disaster it will cause.Future work can be done by extending this simple model to predict rainfall. Since the rainfall is not Gaussian distributed, the task won’t be not as easy as to predict temperature. Generally, daily rainfall follows a mixed gamma distribution, which has two peaks with one peak at zero (because most day has no rainfall), and the other at somewhere larger than zero. Fitting data in this distribution will cause much more computations.]]></content>
      <tags>
        <tag>python, time series, prediction</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[XCESM]]></title>
    <url>%2F2017%2F12%2F25%2FXCESM%2F</url>
    <content type="text"><![CDATA[About xcesmXcesm tries to provide an easy-to-use plugin for xarray to better handle CESM output in python. FeaturesXcesm is still in developing, right now it has the following features: quick plot on global map (quickmap) regrid pop output to linear grids (regrid, nearest interpolation) compute global mean (gbmean, gbmeanpop) diagnose AMOC, PRECP, d18O(only support for iCESM), Heat transport etc. truncate ocean as several main basins (ocean_region) More feature will be added in the future. How to installvia git123git clone https://github.com/Yefee/xcesm.gitcd xcesmpython setup.py install How to useregrid1234567891011import xarray as xrimport xcesmds = xr.open_dataset(&apos;/examples/data/salt.nc&apos;)# defalut to 1x1 degreesalt_rgd = ds.SALT.utils.regrid()print(ds.SALT.shape)(384, 320)print(salt_rgd.shape)(181, 361) quick plot1salt_rgd.utils.quickmap() And moreI don’t have time to write documentation recently, but it will be released in this summer!]]></content>
      <tags>
        <tag>python, CESM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GCM Averager]]></title>
    <url>%2F2017%2F12%2F25%2FGCM-Averager%2F</url>
    <content type="text"><![CDATA[About GCMAveragerGCMAverger is a lightweight post process package designed for large amount general circulation model(GCM) outputs using parallel computing.It was originally designed to post process a long run from the community earth system model (CESM), who genreated hundreds of TBs data.NCAR has developed two fancy tools: pyAverager and pyReshaper, which do the samething as GAMAverager, however, they are pooly maintained and not easy to use. GCMAverager is a xarray-based project, therefore python 2.7, 3.4, 3.5, and 3.6 are supported.Right now, GCMAverager primarily can extract variables from time slice history files into time series files; compute annual (decadal) and seasonal mean for model outputs from time slice or time series files. Installationvia github123git clone https://github.com/Yefee/gcmaverager.gitcd gcmaveragerpython setup.py install via pip1pip install gcmaverager Get startedGCMAverager supports several kinds of average method: ANN (annual mean) MAM (March-April-May, annual mean) JJA (June-July-August, annual mean) SON (September-October-November, annual mean) DJF (December-January-February, annual mean) decadal-ANN (decadal annual mean) decadal-MAM (March-April-May, decadal annual mean) decadal-SON (June-July-August, decadal annual mean) decadal-JJA (September-October-November, decadal annual mean) decadal-DJF (December-January-February, decadal annual mean) TS (extract time series file from original GCM outputs) Extract time series file from time slice filesThis feature only suppports Py 3.x. 123456789101112131415161718192021import gcmaverager as gaimport xarray as xrrootDir = &apos;/Volumes/Chengfei_Data_Center/iTrace/test/&apos;tarDir = &apos;/Volumes/Chengfei_Data_Center/iTrace/output/&apos;prefix = &apos;test&apos;suffix = &apos;0001-0999&apos;method = [&apos;TS&apos;]# get file list and create an xarray objectfl = ga.getFilelist(rootDir)ds = xr.open_mfdataset(fl, decode_times=False, atuoclose=True)# derive time dependent variablesvarList = ds.variables.keys()varList = [v for v in varList if &quot;time&quot; in ds[ v].dims and len(ds[v].dims) &gt; 2]# feed it to GAMAveragerfl = [ds[var] for var in varList]ga.averager(fl, tarDir, prefix, suffix, method) The outputs are in tarDir, which has pattern prefix+variable+suffix+’.nc’(e.g. test.TEMP.0001-0999.nc) Compute average from time slice filesThis feature only suppports Py 3.x. 12345678910111213141516171819202122import gcmaverager as gaimport xarray as xrrootDir = &apos;/Volumes/Chengfei_Data_Center/iTrace/test/&apos;tarDir = &apos;/Volumes/Chengfei_Data_Center/iTrace/output/&apos;prefix = &apos;test&apos;suffix = &apos;0001-0999&apos;method = [&apos;ANN&apos;, &apos;decadal-ANN&apos;]# get file list and create an xarray objectfl = ga.getFilelist(rootDir)ds = xr.open_mfdataset(fl, decode_times=False, atuoclose=True)# derive time dependent variablesvarList = ds.variables.keys()varList = [v for v in varList if &quot;time&quot; in ds[ v].dims and len(ds[v].dims) &gt; 2]# feed it to GAMAveragerfl = [ds[var] for var in varList]ga.averager(fl, tarDir, prefix, suffix, method) The outputs are in tarDir, which has pattern prefix+variable+suffix+’ANN.nc’(e.g. test.TEMP.0001-0999.ANN.nc) Compute average from time series files1234567891011121314import gcmaverager as garootDir = &apos;/Volumes/Chengfei_Data_Center/iTrace/test/&apos;tarDir = &apos;/Volumes/Chengfei_Data_Center/iTrace/output/&apos;prefix = &apos;test&apos;suffix = &apos;0001-0999&apos;method = [&apos;ANN&apos;, &apos;decadal-ANN&apos;]# get file list and create an xarray objectfl = ga.getFilelist(rootDir)# feed it to GAMAveragerga.averager(fl, tarDir, prefix, suffix, method)]]></content>
      <tags>
        <tag>python, CESM, Climate Analysis</tag>
      </tags>
  </entry>
</search>
